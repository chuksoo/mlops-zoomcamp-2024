\documentclass[letterpaper,12pt,notitlepage,twoside]{report}

\newcommand{\watermark}{MLOps}
\newcommand{\booktopic}{Machine Learning Operations}
\newcommand{\lastname}{Okoli}
\usepackage{NoteStyle}
\usepackage{placeins}
\usepackage{listings}
\usepackage{pifont}

\definecolor{firebrick}{rgb}{0.7, 0.13, 0.13}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

% Prevents hypenation 
\tolerance=1
\emergencystretch=\maxdimen
\hbadness=10000


\lstset{style=mystyle}

\title{Machine Learning Operations}
\author{Chuks Okoli}
\date{\footnotesize Last Updated: \today}

\hypersetup{
  pdftitle={Machine Learning Operations},
  pdfauthor={Chuks Okoli},
  pdfsubject={},
}

% ToC on the title page:
% https://tex.stackexchange.com/a/45863
\makeatletter
\newcommand*{\toccontents}{\@starttoc{toc}}
\makeatother

\begin{document}

\begin{titlepage}
  \pagestyle{plain}
  \maketitle
  \setcounter{tocdepth}{2}

  \pdfbookmark[section]{\contentsname}{toc}
  \toccontents
  % Separate page:
  % \tableofcontents
\end{titlepage}

%------------------- Chapter 1: Introduction  -----------------%
\chapter{Introduction} \label{ch:1}

\firstletter{Hello there}, and welcome to \booktopic. This work is a culmination of hours of effort to create my reference for machine learning operations. All of the explanations are in my own words but majority of the content are based on Alexey Grigorev's DataTalksClub \href{https://github.com/DataTalksClub/mlops-zoomcamp}{MLOps Zoomcamp course}.

\begin{mathaside}[frametitle=\mathtitle{Explaining MLOps to a \color{firebrick}{5 year old}}]
\textit{Imagine you have a magical garden. In this garden, you have a special plant that can grow different kinds of fruits, but you need to take good care of it.}

\textit{MLOps is like having a magical gardener to help you. This gardener does three important things:}
\begin{enumerate}
    \item \textit{\textbf{Teaching the Plant}: The gardener shows the plant pictures of different fruits (like apples, oranges, and bananas) so it can learn to grow them. This is like the gardener helping the plant understand what to do. In MLOps, this is called training a machine learning model.}
    
    \item \textit{\textbf{Taking Care of the Garden}: The gardener makes sure the soil is rich, the water is just right, and there are no weeds. The gardener also provides the plant with the best tools and instructions to grow healthy fruits. In MLOps, this is called managing the infrastructure for the machine learning model.}
    
    \item \textit{\textbf{Sharing the Fruits}: Once the plant grows the fruits, the gardener picks them and puts them in baskets for everyone to enjoy. The gardener makes sure the fruits are easy to find and delicious. In MLOps, this is called deploying the model so it can be used for something useful.}
\end{enumerate}

\textit{Here's the magical part: The gardener watches over the garden to make sure everything is running smoothly. If a bug tries to eat the plant or if the plant stops growing fruits, the gardener fixes the problem right away.}

\textit{So, MLOps is like having a magical gardener who helps your special plant (machine learning model) stay happy, healthy, and productive, making sure it grows the best fruits for everyone to enjoy!}
\end{mathaside}


\section{What is MLOps?}
\inspiration{MLOps, also known as DevOps for machine learning, is an umbrella term that encompasses philosophies, practices, and technologies that are related to implementing machine learning lifecycles in a production environment.}{Microsoft Blog}{}

Machine Learning Operations (MLOps) is a set of best practices for putting machine learning models into production. The process for a machine learning project involves:
\begin{itemize}
	\item Design - define if machine learning is the right tool for solving the problem
	\item Train - train the model to find the best possible model
	\item Operate - deploy the model,  and monitor degradation or quality of the model
\end{itemize}
MLOps is a set of practices for automating everything and working together as a team on a machine learning project.

 \begin{funfact}[frametitle=\facttitlep{Fun Fact}{MLOps Principles}]
As machine learning and AI become more common in software, it's important to create guidelines and tools for testing, deploying, managing, and monitoring ML models in real-world use. This is where MLOps comes in. It helps prevent ``technical debt'' in machine learning projects by ensuring smooth operation and maintenance of models throughout their lifecycle.
\end{funfact}

MLOps helps to manage and orchestrate the end-to-end machine learning lifecycle by ensuring models are consistently accessible, reproducible, and scalable. It focuses on automating deployment and monitoring of ML pipelines while optimizing the model development process.

The three-phase approach to implementing machine learning (ML) solutions are:
\begin{itemize}
\item \textbf{Business Understanding and Design}: This phase involves identifying user needs, designing ML solutions to address them, and assessing project development. Prioritizing ML use cases and defining data requirements are key steps. The architecture of the ML application, serving strategy, and testing suite are designed based on functional and non-functional requirements.
\item \textbf{ML Experimentation and Development}: This phase focuses on verifying the applicability of ML by implementing Proof-of-Concept models. It involves iteratively refining ML algorithms, data engineering, and model engineering to deliver a stable, high-quality ML model for production.
\item \textbf{ML Operations}: Here, the emphasis is on deploying the developed ML model into production using established DevOps practices. Testing, versioning, continuous delivery, and monitoring are essential aspects of this phase.
\end{itemize}
These phases are interconnected, with design decisions influencing experimentation and deployment options.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/Iterative MLOps.png}
	\caption{The complete MLOps process includes three broad phases of ``Designing the ML-powered application'', ``ML Experimentation and Development'',  and ``ML Operations''.}
	\label{fig:1}
\end{figure}
\FloatBarrier

MLOps leverages various tools to simplify the machine learning lifecycle.
\begin{itemize}[label={--}]
\item \textbf{Machine learning frameworks} like Kubernetes, TensorFlow and PyTorch for model development and training.
\item \textbf{Version control systems} like Git for code and model version tracking.
\item \textbf{CI/CD tools} such as Jenkins or GitLab CI/CD for automating model building, testing and deployment.
\item \textbf{MLOps platforms} like Kubeflow and MLflow manages model lifecycles, deployment and monitoring.
\item \textbf{Cloud computing platforms} like AWS, Azure and IBM Cloud provide scalable infrastructure for running and managing ML workloads.
\end{itemize}


\section{Environment Preparation}
\subsection{Configuring Environment with GitHub Codespaces}
To configure the environment using GitHub codespaces, first create a repository on GitHub, give the repository a name, add a ``README'' file and a \texttt{.gitignore} template, choose a license and create the repo. In the repo main page, click on \texttt{Create codespace on main} as shown in \textbf{Fig. \ref{fig:2}}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/Codespaces.png}
	\caption{GitHub Codespaces setup}
	\label{fig:2}
\end{figure}
\FloatBarrier

A connection to Visual Studio Code can be made through Codespaces.  Start a new terminal, change directory to the workspaces and download and install Anaconda distribution of python in it using the step below.
 \begin{itemize}
\item \textbf{Step 1}: Download and install the Anaconda distribution of Python 
\begin{lstlisting}[language=python, numbers=none]
wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
\end{lstlisting}
\item \textbf{Step 2}: Run this command: 
\begin{lstlisting}[language=python, numbers=none]
bash Anaconda3-2022.05-Linux-x86_64.sh
\end{lstlisting}

After installing Anaconda, initialize it. In a new terminal, confirm that Anaconda is running in the workspaces
\begin{lstlisting}[language=python, numbers=none]
(base) @your_username -> /workspaces/mlops-zoomcamp-2024 (main) $ which python
/home/codespace/anaconda3/bin/python
\end{lstlisting}
\item \textbf{Step 3}: Make sure to install \texttt{pyarrow} in order to download parquet file
\begin{lstlisting}[language=python, numbers=none]
!pip install pyarrow
\end{lstlisting}
\item \textbf{Step 4}: Run jupyter notebook
\begin{lstlisting}[language=python, numbers=none]
jupyter notebook
\end{lstlisting}
\end{itemize}

\subsection{Configuring Environment with Amazon Web Service (AWS)}
To install using AWS,  create an account in AWS, go to EC2 and create an instance. Select the OS to use e.g. Ubuntu with 64-bit(x86) architecture. Select the instance type that will be sufficient for your project. Configure the cloud resources and launch. \\
Recommended development environment: Linux

 \begin{itemize}
\item \textbf{Step 1}: Download and install the Anaconda distribution of Python 
\begin{lstlisting}[language=python, numbers=none]
wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
bash Anaconda3-2022.05-Linux-x86_64.sh
\end{lstlisting}
\item \textbf{Step 2}: Update existing packages
\begin{lstlisting}[language=python, numbers=none]
sudo apt update
\end{lstlisting}
\item \textbf{Step 3}: Install Docker
\begin{lstlisting}[language=python, numbers=none]
sudo apt install docker.io
\end{lstlisting}
To run docker without sudo:
\begin{lstlisting}[language=python, numbers=none]
sudo groupadd docker
sudo usermod -aG docker \$USER
\end{lstlisting}
\item \textbf{Step 4}: Install Docker Compose \\
Install docker-compose in a separate directory
\begin{lstlisting}[language=python, numbers=none]
mkdir soft
cd soft
\end{lstlisting}
To get the latest release of Docker Compose, go to https://github.com/docker/compose and download the release for your OS.
\begin{lstlisting}[language=python, numbers=none]
wget https://github.com/docker/compose/releases/download/v2.5.0/docker-compose-linux-x86_64 -O docker-compose
\end{lstlisting}
Make it executable
\begin{lstlisting}[language=python, numbers=none]
chmod +x docker-compose
\end{lstlisting}
Add the soft directory to PATH. Open the .bashrc file with nano:
\begin{lstlisting}[language=python, numbers=none]
nano ~/.bashrc
\end{lstlisting}
In .bashrc, add the following line:
\begin{lstlisting}[language=python, numbers=none]
export PATH="${HOME}/soft:${PATH}"
\end{lstlisting}
Save it and run the following to make sure the changes are applied:
\begin{lstlisting}[language=python, numbers=none]
source ~/.bashrc
\end{lstlisting}
\item \textbf{Step 5}: Run Docker
\begin{lstlisting}[language=python, numbers=none]
docker run hello-world
\end{lstlisting}
\end{itemize}

\section{Model Development}
\subsection{Taxi trip prediction with machine learning}
    \begin{example}[frametitle=\extitle{TLC Trip Prediction}]
      In this section, we use machine learning to predict the trip duration for trips in New York. The data used were collected from the \href{https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page}{NYC Taxi and Limousine Commission (TLC)}. This dataset contains yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data is stored in the PARQUET format.  We built a simple linear regression model and save the model as a pickle file for subsequent use later one.  See the \href{https://github.com/chuksoo/mlops-zoomcamp-2024/blob/main/01%20-%20Intro/duration-prediction.ipynb}{duration prediction notebook}.
    \end{example}

We can download parquet file using:
\begin{lstlisting}[language=python, numbers=none]
# download parquet data
!wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet
\end{lstlisting}

\section{MLOps Maturity Model}
The MLOps maturity model helps clarify the Development Operations (DevOps) principles and practices necessary to run a successful MLOps environment.  The maturity is the extent to which MLOps is implemented in a team. The MLOps maturity model encompasses five levels of technical capability.

% Please add the following required packages to your document preamble:
\begin{table}[H]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cp{2.5cm}p{7.0cm}p{7.0cm}}
\hline
Level & Description                     & Highlights                                                                                                                                                                                                                                         & Technology                                                                                                                                                                                   \\ \hline \hline
0     & No MLOps & \begin{itemize}[noitemsep, topsep=0pt]
    \item Hard to manage lifecycle
    \item Disparate teams
    \item Systems as ``black boxes''
\end{itemize}                 & \begin{itemize}[noitemsep, topsep=0pt]
    \item Manual builds/deployments
    \item Manual testing
    \item No centralized tracking
\end{itemize} \\ \hline
1     & DevOps but no MLOps            & \begin{itemize}[noitemsep, topsep=0pt]
   \item Less painful releases
    \item Limited feedback
    \item Hard to reproduce results
\end{itemize}              & \begin{itemize}[noitemsep, topsep=0pt]
    \item Automated builds
    \item Automated tests for application code
\end{itemize} \\ \hline
2     & Automated Training              & \begin{itemize}[noitemsep, topsep=0pt]
   \item Managed, traceable training
    \item Easy to reproduce model
    \item Low friction releases
\end{itemize}                      & \begin{itemize}[noitemsep, topsep=0pt]
    \item Automated model training
    \item Centralized tracking of model training performance
    \item Model management
\end{itemize} \\ \hline
3     & Automated Deployment     & \begin{itemize}[noitemsep, topsep=0pt]
    \item Releases are low friction and automatic
    \item Full traceability from deployment back to original data
    \item Entire environment managed: train > test > production
\end{itemize}                            & \begin{itemize}[noitemsep, topsep=0pt]
   \item Integrated A/B testing
    \item Automated tests for all code
    \item Centralized tracking
\end{itemize} \\ \hline
4     & Full MLOps Automated Operations & \begin{itemize}[noitemsep, topsep=0pt]
    \item Fully automated, monitored
    \item Systems provide improvement information
    \item Zero-downtime
\end{itemize} & \begin{itemize}[noitemsep, topsep=0pt]
    \item Automated training/testing
    \item Centralized metrics
\end{itemize}                                                              \\ \hline \hline
\end{tabular}%
}
\caption{Machine Learning operations maturity model (culled from \href{https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/mlops-maturity-model}{Microsoft Blog}}
\label{tab:my-table}
\end{table}

%--------------- Chapter 2: Experiment Tracking and Model Management -----------%
\chapter{Experiment Tracking and Model Management} \label{ch:2}

\inspiration{MLOps is critical for the success of AI projects because it allows teams to iterate quickly and deploy machine learning models reliably and at scale.}{Andrew Ng}{}

\section{Introduction to Experiment Tracking}

 \begin{funfact}[frametitle=\facttitlep{Fun Fact}{MLflow}]
MLflow is an open-source platform that helps manage the end-to-end machine learning lifecycle. It provides a set of tools to streamline and automate various stages of the machine learning process, from experimentation to deployment. 
\end{funfact}

Machine Learning experiment is the process of building an ML model. Experiment run represents each trial in an ML experiment. A run artifact is any file associated with an ML run. The experiment metadata is all the information related to the experiment. \emph{Experiment tracking} is the process of keeping track of all the \textbf{relevant information} from an \textbf{ML experiment}, which includes:
\begin{itemize}[noitemsep, topsep=0pt]
	\item Source code
	\item Environment
	\item Data
	\item Models
	\item Hyperparameters
	\item Metrics
\end{itemize}

    \begin{mathaside}[frametitle=\mathtitle{Why is Experiment Tracking \color{firebrick}{so important}}]
      Experiment tracking is important because of \textbf{Reproducibility}, \textbf{Organization}, and \textbf{Optimization}
    \end{mathaside}

Experiment tracking is done using MLflow. MLflow is ``an open source platform for the machine learning lifecycle''. In practice, it's just a Python package that can be installed with pip, and it contains four main modules:
\begin{itemize}[noitemsep, topsep=0pt]
	\item Tracking
	\item Models
	\item Model Registry
	\item Projects
\end{itemize}

The MLflow Tracking module allows you to organize your experiments into runs, and to keep track of:
\begin{itemize}[noitemsep, topsep=0pt]
	\item Parameters
	\item Metrics
	\item Metadata
	\item Artifacts
	\item Models
\end{itemize}

Along with this information, MLflow automatically logs extra information about the run:
\begin{itemize}[noitemsep, topsep=0pt]
	\item Source code i.e., the name of the file that was used to run the experiment
	\item Version of the code (git commit)
	\item Start and end time
	\item Author
\end{itemize}

\subsection{How MLflow Helps Manage Machine Learning Experiments}
Here are some key ways in which MLflow can help with machine learning experiments:
\subsubsection{Experiment Tracking}
MLflow Tracking allows you to log and query experiments using APIs or a web-based interface. Key features include:

\begin{itemize}
    \item \textbf{Log Parameters and Metrics:} Easily log hyperparameters, metrics, and artifacts (such as model files) for each run.
    \item \textbf{Organize and Compare Runs:} Organize runs by experiments and compare results visually.
    \item \textbf{Search and Query:} Search and filter experiments using a web UI or API, allowing you to quickly find runs with specific attributes.
\end{itemize}

\subsubsection{Reproducibility}
MLflow promotes reproducibility of experiments by providing:

\begin{itemize}
    \item \textbf{Code Versioning:} Integrates with version control systems (like Git) to log the version of the code that produced a particular run.
    \item \textbf{Environment Management:} Capture and reproduce the software environment using Conda environments or Docker images.
    \item \textbf{Artifacts:} Store and retrieve artifacts such as datasets, models, and images, ensuring all elements of an experiment are preserved.
\end{itemize}

\subsubsection{Model Management}
MLflow Models facilitate model packaging, sharing, and deployment:

\begin{itemize}
    \item \textbf{Standardized Format:} Save models in a standardized format that includes the model itself along with its dependencies and environment.
    \item \textbf{Multi-Platform Support:} Export models to various formats (e.g., TensorFlow, PyTorch, scikit-learn) and deploy them to different environments (e.g., cloud services, Docker).
    \item \textbf{Model Registry:} Register and version models, track model lineage, and transition models through stages (e.g., ``staging'', ``production'').
\end{itemize}

\subsubsection{Deployment}
MLflow provides tools for deploying models to various platforms:

\begin{itemize}
    \item \textbf{Built-in Deployment Options:} Deploy models to cloud platforms like AWS SageMaker, Azure ML, and Google Cloud ML Engine.
    \item \textbf{Custom Deployments:} Create custom deployment logic using the MLflow REST API or the command-line interface (CLI).
    \item \textbf{Batch and Real-Time Serving:} Support for both batch and real-time serving of models, enabling various deployment scenarios.
\end{itemize}

\subsubsection{Collaboration}
MLflow facilitates collaboration among team members by providing:

\begin{itemize}
    \item \textbf{Centralized Tracking Server:} A centralized tracking server where team members can log and view experiment runs.
    \item \textbf{Sharing and Collaboration:} Share results, models, and insights easily within the team, fostering better collaboration and knowledge sharing.
    \item \textbf{Integration with CI/CD:} Integrate MLflow with continuous integration and continuous deployment (CI/CD) pipelines for automated testing and deployment of models.
\end{itemize}

\subsubsection{Scalability}
MLflow is designed to scale with your needs:

\begin{itemize}
    \item \textbf{Distributed Tracking:} Support for tracking experiments across distributed environments, making it suitable for large-scale machine learning projects.
    \item \textbf{Flexible Storage Options:} Use various backend storage systems for logging data, including file systems, databases, and cloud storage.
\end{itemize}

By integrating MLflow into your machine learning workflow, you can enhance the management, reproducibility, and deployment of your experiments. It provides a comprehensive set of tools that streamline the entire lifecycle of machine learning models, making it easier to track, reproduce, and deploy models at scale.


\section{Experiment tracking with MLflow}
To start experiment tracking, we need to create a conda environment for tracking experiment and activate it.
\begin{lstlisting}[language=python, numbers=none]
conda create -n exp-tracking-env python=3.9
conda activate exp-tracking-env
\end{lstlisting}

Once the environment is activated, install the ``requirements.txt'' file using:
\begin{lstlisting}[language=python, numbers=none]
pip install -r requirements.txt
\end{lstlisting}

Check the installed packages using:
\begin{lstlisting}[language=python, numbers=none]
pip list
\end{lstlisting}

To launch mlflow, and store all the artifacts in an sqlite database, we run:
\begin{lstlisting}[language=python, numbers=none]
mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001
\end{lstlisting}

If you encounter error when launching mlflow, use:
\begin{lstlisting}[language=python, numbers=none]
ps -A | grep gunicorn
\end{lstlisting} to view the processes using the port and kill them with:
\begin{lstlisting}[language=python, numbers=none]
kill <process number>
\end{lstlisting} before re-launching mlflow. 


To access mlflow ui, open \href{http://127.0.0.1:5001} in your browser. The mlflow interface is seen in \textbf{Fig. \ref{fig:3}}.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/mlflow-experiment-interface.png}
	\caption{MLflow Experiment Interface}
	\label{fig:3}
\end{figure}
\FloatBarrier

The updated notebook with experiment tracking using MLflow and logged predictions in MLflow UI is shown in this \href{https://github.com/chuksoo/mlops-zoomcamp-2024/blob/main/02%20-%20Experiment%20Tracking/duration-prediction.ipynb}{notebook for experiment tracking with mlflow}.

\section{Machine Learning Lifecycle}
 \begin{funfact}[frametitle=\facttitlep{Fun Fact}{MLOps cycle}]
The Machine Learning lifecycle refers to the multiple steps that are needed to build and maintain a machine learning model.
\end{funfact}

In Machine Learning Lifecycle,  we train some model, we tuned the hyperparameters, we evaluated the model and then logged some metrics, hyperparameters and other information needed to mlflow.  Once we finish with this experiment tracking stage, it means that we are happy with the model. Then we can start thinking of saving this model and have some kind of versioning. After that, we would like to deploy the model. We may realize that the model needs to be updated in order to scale. Finally, once we deploy the model, the prediction monitoring stage starts.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/neptune-mlops.png}
	\caption{MLOps cycle and machine learning experiment tracking}
	\label{fig:4}
\end{figure}
%\FloatBarrier

\subsection{Logging models in MLflow}
Two options :
\begin{itemize}[noitemsep, topsep=0pt]
\item Log model as an artifact
\begin{lstlisting}[language=python, numbers=none]
mlflow.log_artifact("<mymodel>", artifact_path="models/")
\end{lstlisting}
\item Log model using the method ``log\_model''
\begin{lstlisting}[language=python, numbers=none]
mlflow.<framework>.log_model(model, artifact_path="models/")
\end{lstlisting}
\end{itemize}

\subsection{Model Management}
Model management is a critical aspect of the machine learning lifecycle, encompassing the processes and tools used to effectively organize, version, and track machine learning models. Effective model management ensures that models can be deployed, monitored, and updated seamlessly, maintaining their performance and relevance over time. The main components of model management:

\subsubsection*{1. Versioning}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Importance:} Versioning allows data scientists to keep track of different iterations of a model, ensuring that they can reproduce results and compare performance across versions.
    \item \textbf{Tools:} Tools like MLflow, DVC, and Git provide functionalities to version models, track changes, and manage model metadata.
\end{itemize}

\subsubsection*{2. Deployment}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Purpose:} Deployment involves taking a trained model and making it available for inference in a production environment.
    \item \textbf{Methods:} Models can be deployed as a python function, in a docker container, as an APIs, embedded in applications, as a batch job in Apache Spark or integrated into data pipelines. Platforms like Kubernetes, AWS SageMaker, and Azure ML facilitate seamless deployment.
\end{itemize}

\subsubsection*{3. Monitoring}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Need:} Continuous monitoring of models in production is essential to ensure they perform as expected and to detect any performance degradation or data drift.
    \item \textbf{Metrics:} Key metrics to monitor include accuracy, latency, throughput, and error rates. Tools like Prometheus, Grafana, and custom logging solutions can be used for monitoring.
\end{itemize}

\subsubsection*{4. Retraining}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Process:} As new data becomes available, models may need to be retrained to maintain their performance.
    \item \textbf{Automation:} Automated retraining pipelines can be set up to periodically retrain models, incorporating the latest data and ensuring that the model remains up-to-date.
\end{itemize}

\subsubsection*{5. Governance and Compliance}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Compliance:} Ensuring that models comply with regulatory standards and organizational policies is crucial, especially in industries like finance and healthcare.
    \item \textbf{Documentation:} Proper documentation and audit trails are necessary for compliance and to provide transparency into how models are developed and used.
\end{itemize}

\subsubsection*{6. Collaboration}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Teamwork:} Effective model management facilitates collaboration among data scientists, engineers, and business stakeholders.
    \item \textbf{Tools:} Collaborative tools like Jupyter Notebooks, GitHub, and MLflow allow teams to share code, experiments, and insights.
\end{itemize}

\subsection*{Benefits of Effective Model Management}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Reproducibility:} Ensures that models can be consistently reproduced, which is crucial for validation and debugging.
    \item \textbf{Scalability:} Facilitates the scaling of machine learning efforts across an organization.
    \item \textbf{Efficiency:} Streamlines the deployment and monitoring processes, reducing time to market for new models.
    \item \textbf{Compliance:} Helps maintain compliance with legal and regulatory requirements.
\end{itemize}

In summary, model management is essential for maintaining the lifecycle of machine learning models, from development through deployment and monitoring, ensuring they continue to deliver value and perform optimally in production environments.

\subsection{Model Registry in Machine Learning}

    \begin{mathaside}[frametitle=\mathtitle{Why Model \color{firebrick}{registry}}]
            Model registry is an essential tool for managing the lifecycle of machine learning models, ensuring that they are well-organized, reproducible, and efficiently deployable, while enhancing collaboration and compliance within an organization.
    \end{mathaside}

A model registry is a centralized repository that stores and manages machine learning models. It plays a crucial role in the machine learning lifecycle by providing a systematic way to organize, version, and track models. The key components and benefits of a model registry:

\textbf{1. Versioning}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Purpose:} Keeps track of different iterations and versions of a model.
    \item \textbf{Functionality:} Allows comparison of model performance over time and ensures reproducibility.
\end{itemize}

\textbf{2. Metadata Management}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Purpose:} Stores important information about models, such as hyperparameters, training data, and evaluation metrics.
    \item \textbf{Functionality:} Facilitates model auditability and governance.
\end{itemize}

\textbf{3. Lifecycle Management}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Purpose:} Manages the lifecycle of models from development to deployment and monitoring.
    \item \textbf{Functionality:} Ensures smooth transitions between different stages of the model lifecycle.
\end{itemize}

\textbf{4. Access Control}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Purpose:} Defines who can access and modify models in the registry.
    \item \textbf{Functionality:} Enhances security and ensures that only authorized users can make changes.
\end{itemize}

\textbf{5. Deployment Integration}
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Purpose:} Facilitates the deployment of models to production environments.
    \item \textbf{Functionality:} Provides integration with deployment tools and platforms for seamless model serving.
\end{itemize}

As we continue to generate more models, we need a model registry consisting of a tracking server to keep track of the models. Once you've decided that some of the model are ready for production, you then register the model into the mlflow model registry. Within the model registry, we have a staging , production and archive area for the models. In the model registry, all the models that are ready for production are stored here. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/model registry.png}
	\caption{Model Registry}
	\label{fig:5}
\end{figure}
\FloatBarrier

The MLOps Engineer can look at the models in the registry, inspect their hyperparameters, the size of the model and so on. He can then decide to move the models between the different stages. The model that is ready for production can be assigned to ``staging'', the current model that is used in production can be assigned to the ``production'' stage, and some of the models can be archived in the ``archived'' stage. The archived models can be retrieved from the archived stage and move them back to production if we want to roll back some deployments. The model registry is not deploying any model, it just lists the models that are production ready and the stages are just labels assigned to the model. The model registry will need to be complemented with some CI/CD code in order to do the actual deployment of the models. 

    \begin{mathaside}[frametitle=\mathtitle{Using \color{firebrick}{MLflowClient}}]
           \texttt{mlflow.client} is a module in MLflow that provides a way to interact programmatically with an MLflow tracking server. The primary class in this module is MlflowClient, which provides a comprehensive API for managing experiments, managing runs, models, logging artifacts, and interacting with model registry. 
    \end{mathaside}

\subsection{Interacting Programmatically with MLflowClient}
The \texttt{mlflow.client} module is useful for advanced use cases where you need programmatic control over MLflow entities, especially in production environments where automated workflows and integrations are required.

Here's a basic example demonstrating how to use \texttt{MlflowClient}:

\begin{lstlisting}[language=Python, caption=Example Usage of \texttt{MlflowClient}, label=code:example]
import mlflow
from mlflow.tracking import MlflowClient

# Initialize the client
client = MlflowClient()

# Create an experiment
experiment_id = client.create_experiment("My New Experiment")

# Start a new run in the experiment
run = client.create_run(experiment_id)
run_id = run.info.run_id

# Log parameters, metrics, and tags
client.log_param(run_id, "param1", 5)
client.log_metric(run_id, "metric1", 0.89)
client.set_tag(run_id, "tag1", "value1")

# Log an artifact (a file)
with open("output.txt", "w") as f:
    f.write("Hello, world!")
client.log_artifact(run_id, "output.txt")

# End the run
client.set_terminated(run_id)

# Get information about the run
run_info = client.get_run(run_id)
print(run_info)
\end{lstlisting}

\subsubsection{Key Methods of \texttt{MlflowClient}}

\subsubsection*{1. Experiment Management}
\begin{itemize}
    \item \texttt{create\_experiment(name, artifact\_location=None, tags=None)}: Creates a new experiment.
    \item \texttt{get\_experiment(experiment\_id)}: Retrieves an experiment by ID.
    \item \texttt{delete\_experiment(experiment\_id)}: Deletes an experiment.
\end{itemize}

\subsubsection*{2. Run Management}
\begin{itemize}
    \item \texttt{create\_run(experiment\_id, start\_time=None, tags=None)}: Starts a new run.
    \item \texttt{log\_param(run\_id, key, value)}: Logs a parameter.
    \item \texttt{log\_metric(run\_id, key, value, timestamp=None, step=None)}: Logs a metric.
    \item \texttt{set\_tag(run\_id, key, value)}: Sets a tag.
    \item \texttt{log\_artifact(run\_id, local\_path, artifact\_path=None)}: Logs an artifact.
\end{itemize}

\subsubsection*{3. Model Registry Management}
\begin{itemize}
    \item \texttt{create\_registered\_model(name)}: Creates a new registered model.
    \item \texttt{create\_model\_version(name, source, run\_id, tags=None, run\_link=None, description=None)}: Creates a new model version.
    \item \texttt{transition\_model\_version\_stage(name, version, stage)}: Transitions a model version to a new stage.
\end{itemize}

\section{MLflow in Practice}
\subsection{Different scenarios for running MLflow}
Let's consider these three scenarios:
\begin{itemize}[noitemsep, topsep=0pt]
\item \textbf{\smtt{A single data scientist participating in an ML competition}}: In this scenario, having remote tracking server will be an overkill. Saving this information locally will be enough. Also, using model registry is useless since the data scientist is not deploying this model to production.
\item \textbf{\smtt{A cross-functional team with one data scientist working on an ML model}}:Here, sharing the experiment information is important. Also, using model registry will be a good idea but it can be run remotely or local host.
\item \textbf{\smtt{Multiple data scientists working on multiple ML models}}: Since multiple data scientist are working on multiple model, collaboration and sharing experiment information is very important. One data scientist can build the model, another data scientist can tune different hyperparameters to add to the model. They need a way to keep track of the models using a remote tracking server. Also, it is important to manage the lifecycle of the model since multiple people build and deploy the model hence model registry is important. 
\end{itemize}

\subsection{Things to Consider before Configuring MLflow}
There are different things to consider before configuring MLflow.
\begin{itemize}[noitemsep, topsep=0pt]
\item \textbf{Backend Store}: Where MLflow saves information about your experiement such as metadata, models etc
	\begin{itemize}[label={\ding{224}}]
		\item local filesystem
		\item SQLAlchemy compatible DB (e.g. SQLite)
	\end{itemize}
\item \textbf{Artifacts Store}: Decide where to store the artifact i.e., locally or remote
	\begin{itemize}[label={\ding{224}}]
		\item local filesystem
		\item remote (e.g.  s3 bucket)
	\end{itemize}
\item \textbf{Tracking Server}: Decide how to run tracking server
	\begin{itemize}[label={\ding{224}}]
		\item no tracking server
		\item localhost
		\item remote
	\end{itemize}
\end{itemize}

\subsection{Setting Up MLflow Tracking Server in AWS}
Assuming we have multiple data scientist working on multiple ML models. We can setup a remote tracking server on \href{https://aws.amazon.com/ec2/}{Amazon Elastic Compute Cloud (Amazon EC2)} with PostgreSQL backend using \href{https://aws.amazon.com/rds/}{Amazon Relational Databse Service (RDS)} and \href{https://aws.amazon.com/s3/}{Amazon Simple Storage Service (S3)} to store the model and artifacts. MLflow uses the backend store and artifact store for persistent runs. The PostgreSQL backend stores metadata such as parameters, metrics, and tags while the artifact store holds large files such as serialized models and config files. MLflow experiments can be explored by accessing the remote server. To run experiments using AWS, we have to configure an AWS account.

\subsubsection{Basic AWS Setup}

 \begin{itemize}
\item \textbf{Step 1}: Launch a new EC2 instance \\
For this, you can select one of the instance types that are free tier eligible. For example, we will select an Amazon Linux OS (\smtt{Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type}) and a \smtt{t2.micro} instance type, which are free tier eligible.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/EC2-setup.png}
	\caption{Application and OS Setup}
	\label{fig:6}
\end{figure}
\FloatBarrier
You'll also need to create a new key pair so later you can connect to the new instance using SSH. Click on ``Create new key pair'' and complete the details like in the image below:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{Images/mlflow-key-pair.png}
	\caption{Create Key Pair}
	\label{fig:7}
\end{figure}
\FloatBarrier

Select the new key pair and then click on ``Launch Instance''.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{Images/key-pair-setup.png}
	\caption{Select Key Pair}
	\label{fig:8}
\end{figure}
\FloatBarrier
 Leave the rest of the configuration the way it is an launch the instance.

\item \textbf{Step 2}:  Configure the security group \\
Open up the new instance and select the newly created ``Security groups'' in ``Security''. Edit security group so the EC2 instance accepts SSH (port 22) and HTTP connections (port 5000). Do this by ``Edit Inbound rules''. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{Images/security-group.png}
	\caption{Security group setup}
	\label{fig:9}
\end{figure}
\FloatBarrier

\item \textbf{Step 3}: Create an S3 bucket \\
The Amazon Simple Storage Service (S3) will be used as the artifact store. Go to ``S3'' and click on ``Create bucket''. Fill in the bucket name as in the image below and leave all the other configurations with their default values.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{Images/S3-setup.png}
	\caption{S3 bucket setup}
	\label{fig:10}
\end{figure}
\FloatBarrier

\item \textbf{Step 4}: Create a new PostgreSQL database in RDS \\
The Amazon Relational Databse Service (RDS) uses PostgreSQL engine as the backend store. Go to the RDS Console and click on ``Create database''. Make sure to select ``PostgreSQL'' engine type and the ``Free tier'' template.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/create-database.png}
	\caption{Create RDS Database}
	\label{fig:11}
\end{figure}
\FloatBarrier

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/RDS-setup.png}
	\caption{Setup RDS username and password}
	\label{fig:12}
\end{figure}
\FloatBarrier

Select a name for your DB instance, set the master username as ``mlflow'' and tick the option ``Auto generate a password'' so Amazon RDS generate a password automatically.  Setup instance connection, use the default VPC and connectivity to the compute resource. Use default values for all the other configurations.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/instance-configuration.png}
	\caption{Instance Configuration}
	\label{fig:13}
\end{figure}
\FloatBarrier

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/EC2 connectivity to DB.png}
	\caption{EC2 connectivity to DB}
	\label{fig:14}
\end{figure}
\FloatBarrier

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/RDS-setup.png}
	\caption{RDS Setup}
	\label{fig:15}
\end{figure}
\FloatBarrier

Specify a database name so that RDS will create the database for you.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Images/set-db-name.png}
	\caption{Additional configuration to set database name}
	\label{fig:16}
\end{figure}

After clicking on ``launch database'' you will be able to check the newly generated password, but take into account that the automatically generated password will be shown only once!. Take note of the master username, password, initial database name, and endpoint. 


Once the DB instance is created, go to the RDS console, select the new db and under ``Connectivity \& security'' select the VPC security group. Modify the security group by adding a new inbound rule that allows postgreSQL connections on the port 5432 from the security group of the EC2 instance. This way, the server will be able to connect to the postgres database.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{Images/postgresql_inbound_rule.png}
	\caption{Edit Security Groups}
	\label{fig:17}
\end{figure}



\item \textbf{Step 5}: Connect to the EC2 instance and launch the tracking server \\
Go to the EC2 Console and find the instance launched and click on ``Connect'' and then follow the steps described in the tab ``SSH''.

Run the following commands to install the dependencies, configure the environment and launch the server:

\begin{lstlisting}[language=python, numbers=none]
sudo yum update
pip3 install mlflow boto3 psycopg2-binary
aws configure # you'll need to input your AWS credentials here
mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://DB_USER:DB_PASSWORD@DB_ENDPOINT:5432/DB_NAME --default-artifact-root s3://S3_BUCKET_NAME
\end{lstlisting}
Note: before launching the server, check that the instance can access the s3 bucket created. To do that, just run this command from the EC2 instance: \smtt{aws s3 ls}. You should see the bucket listed in the result.

\item \textbf{Step 6}: Access the remote tracking server from your local machine \\
Open a new tab on your web browser and go to this address: \smtt{http://<EC2\_PUBLIC\_DNS>:5000} (you can find the instance's public DNS by checking the details of your instance in the EC2 Console).
\end{itemize}

\section{MLflow: Benefits, limitations and alternatives}
\subsection{Remote tracking server}
The tracking server can be easily deployed to the cloud. Some of the benefits are:
\begin{itemize}[noitemsep, topsep=0pt]
\item Shared experiments with other data scientist
\item Collaborate with others to build and deploy models
\item Give more visibility of the data science efforts
\end{itemize}

\subsection{Issues with running a remote (shared) MLflow server}
Some issues can arise when running a remote MLflow server. They include:
\begin{itemize}[noitemsep, topsep=0pt]
\item Security
	\begin{itemize}[label={\ding{224}}]
	\item Restrict access to the server (e.g. access through VPN)
	\end{itemize}
\item Scalability
	\begin{itemize}[label={\ding{224}}]
	\item Check \href{https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/}{Deploy MLflow on AWS Fargate}
	\item Check \href{https://www.slideshare.net/databricks/mlflow-at-company-scale-239587085}{MLflow at Company Scale}
	\end{itemize}
\item Isolation
	\begin{itemize}[label={\ding{224}}]
	\item Define standard for naming experiments, modles, and a set of default tags
	\item Restrict access to artifacts (e.g. use s3 buckets living in different AWS accounts)
	\end{itemize}
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/remote tracking-mlflow.png}
	\caption{Remote Tracking Server with MLflow}
	\label{fig:6}
\end{figure}
\FloatBarrier

\subsection{MLflow limitations (and when not to use it)}
There are some limitations for MLflow. For example:
\begin{itemize}[noitemsep, topsep=0pt]
\item \textbf{Authetication \& Users:} The open source version of MLflow doesn't provide any sort of authetication.
\item \textbf{Data versioning:} To ensure full reproducibility, we need to version the data used to train the model. MLflow doesn't provide a built-in solution for that but there are a few ways to deal with this limitation.
\item \textbf{Model/Data Monitoring \& Alerting:} This is outside the scope of MLflow and currently there are more suitable tools for doing this.
\end{itemize}

There are some paid alternatives to MLflow including Neptune.ai, Comet, Weight \& Biases and many more. 

%--------------- Chapter 3: Orchestration and ML Pipelines -----------%
\chapter{Orchestration and ML Pipelines} \label{ch:3}
\section{Introduction: ML pipelines and Mage}
\subsection{Operationalizing ML models}
 \begin{funfact}[frametitle=\facttitlep{Fun Fact}{Operationalizing ML models}]
Operationalizing ML models involves moving them from development to production to drive business value
\end{funfact}

To operationalize ML models, we take the following steps:
\begin{itemize}[noitemsep, topsep=0pt]
\item \textbf{\smtt{Preparing the model for deployment}} involves optimizing performance, ensuring it handles real-world data, and packaging it for integration into existing systems.
\item \textbf{\smtt{Deploying the model}} involves moving it from development to production, making it accessible to users and applications.
\item Once deployed, models must be \textbf{\smtt{continuously monitored for accuracy and reliability,}} and may need retraining on new data and updates to maintain effectiveness.
\item \textbf{\smtt{Integrating operationalized model into existing workflows, applications, and decision-making processes}} to drive business impact.
\end{itemize}
Effective operationalization enables organizations to move beyond experimentation and drive tangible value from ML at scale, powering intelligent applications that personalize the customer experience and creates real business value.

\subsection{Why we need to operationalize ML}
\begin{itemize}[noitemsep, topsep=0pt]
\item \textbf{Productivity} - MLOps fosters collaboration between data scientists, ML engineers, and DevOps teams by providing a unified environment for experiment tracking, feature engineering, model management, and deployment. This breaks down silos and accelerates the entire machine learning lifecycle.
\item \textbf{Reliability} - MLOps ensures high-quality, reliable models in production through clean datasets, proper testing, validation, CI/CD practices, monitoring, and governance.
\item \textbf{Reproducibility} - MLOps enables reproducibility and compliance by versioning datasets, code, and models, providing transparency and auditability to ensure adherence to policies and regulations.
\item \textbf{Time-to-value} - MLOps streamlines the ML lifecycle, enabling organizations to successfully deploy more projects to production and derive tangible business value and ROI from AI/ML investments at scale.
\end{itemize}

\subsection{How Mage helps MLOps}
\begin{itemize}[noitemsep, topsep=0pt]
\item \textbf{Data preparation} - Mage offers features to build, run, and manage data pipelines for data transformation and integration, including pipeline orchestration, notebook environments, data integrations, and streaming pipelines for real-time data.
\item \textbf{Training and deployment} - Mage helps prepare data, train machine learning models, and deploy them with accessible API endpoints.
\item \textbf{Standardize complex processes} - Mage simplifies MLOps by providing a unified platform for data pipelining, model development, deployment, versioning, CI/CD, and maintenance, allowing developers to focus on model creation while improving efficiency and collaboration.
\end{itemize}

\subsection{Project setup for Mage}
Clone the following respository containing the complete code for this module:
\begin{lstlisting}[language=python, numbers=none]
git clone https://github.com/mage-ai/mlops.git
\end{lstlisting}

Change directory into the cloned repo:
\begin{lstlisting}[language=python, numbers=none]
cd mlops
\end{lstlisting}

Launch Mage and the database service (PostgreSQL):
\begin{lstlisting}[language=python, numbers=none]
./scripts/start.sh
\end{lstlisting}

If you don't have bash in your enviroment, modify the following command and run it:
\begin{lstlisting}[language=python, numbers=none]
PROJECT_NAME=mlops \
    MAGE_CODE_PATH=/home/src \
    SMTP_EMAIL=$SMTP_EMAIL \
    SMTP_PASSWORD=$SMTP_PASSWORD \
    docker compose up
\end{lstlisting}

The subproject that contains all the pipelines and code is named \texttt{unit\_3\_observability}

\section{Data preparation: ETL and Feature Engineering}
\section{Training: sklearn models and XGBoost}
\section{Observability: Monitoring and Alerting}
\section{Triggering: Inference and Retraining}
\section{Deploying: Running operations in Production}

%--------------- Chapter 4: Deployment -----------%
\chapter{Deployment} \label{ch:4}
\section{Model Deployment}
\subsection{Three ways of deploying models}
 \begin{funfact}[frametitle=\facttitlep{Fun Fact}{Deploying ML models}]
Deploying machine learning model models is an iterative process that often involves multiple rounds of training, testing, and validating before the model is ready for production.
\end{funfact}

Deploying a model means that other application can get predictions from our model. There are three modes of deployment namely \textbf{online} deployment, \textbf{offline} or batch deployment, and \textbf{streaming}.

First, we need to ask ourselves if we need to have predictions immediately or it can wait a little bit for an hour, a day or a week. If it can wait for a little bit, then we go for batch deployment. i.e., the model is not running all the time and we just apply our model to new data regularly. In online deployment, the model is up and running all the time and is always available. Two variant of online deployment is deployment as a web service and deployment via streaming. In web service deployment, we send HTTP requests and the service sends out prediction. In streaming, there is an ``events model service'' listening for events on the stream and reacting to this event. 

\subsection{Web services: Introduction to Flask}
    \begin{mathaside}[frametitle=\mathtitle{Web \color{firebrick}{Service}}]
       A  \texttt{web service} is a method for communicating between two devices over a network using some protocols.
    \end{mathaside}
Assuming we want to use our model inside a \texttt{churn service} in order to make some predictions. The \texttt{marketing service} will communicate with our churn service by sending some request and getting a response. This can be done using a web service. In \textbf{web service}, a user sends a request in the form of a query, then the web service sends back a response to the user with the result.  In \textbf{Fig. \ref{fig:7}}, we have a notebook that was used to train the churn model. The model is saved to file. We can load this model from a different process or web service called the \texttt{churn service}. If the \texttt{marketing service} wants to identify if the user will churn, they send a request to the \texttt{churn service} with information about the user then they get back the predictions and based on these predictions, the marketing service can decide whether they want to send a promotional email with say 25\% discount to prevent churn.  

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/model-serving.png}
	\caption{Serving Churn Model}
	\label{fig:7}
\end{figure}
\FloatBarrier

To do this, we put the model inside a web service using \textbf{flask}, which is a framework for creating web services in python, then we isolate dependencies for this service so they don't interfere with other services on ur machine by creating a special environment for python dependencies using \textbf{Pipenv}. Then we add another layer with system dependencies using \textbf{Docker}, and then finally we deploy the container containing this model to the cloud using \textbf{AWS Elastic Beanstalk}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/layers-to-deployment.png}
	\caption{Layers to deployment}
	\label{fig:8}
\end{figure}
\FloatBarrier

To deploy a model, we turn our notebook to Python script called \texttt{train.py} where we save the model as a pickle file. Then using our \texttt{predict.py}, we can load the model and make prediction. 

There are some methods in web services we can use it to satisfy our problems. Here below we would list some.
\begin{itemize}[noitemsep, topsep=0pt]
 \item \texttt{GET}: GET is a method used to retrieve files, For example when we are searching for a cat image in google we are actually requesting cat images with GET method.
 \item \texttt{POST}: POST is the second common method used in web services. For example in a sign up process, when we are submiting our name, username, passwords, etc we are posting our data to a server that is using the web service. (Note that there is no specification where the data goes)
 \item \texttt{PUT}: PUT is same as POST but we are specifying where the data is going to.
 \item \texttt{DELETE}: DELETE is a method that is used to request to delete some data from the server.
\end{itemize}

We can create a simple service using flask that pings and send a response back. To do that, we create a \texttt{ping.py} file containing:
\begin{lstlisting}[language=python, numbers=none]
from flask import Flask

app = Flask('ping')

@app.route('/ping', methods=['GET'])
def ping():
    return "PONG"

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=9696)
\end{lstlisting}

We start by installing and importing flask
\begin{lstlisting}[language=python, numbers=none]
from flask import Flask
\end{lstlisting}

We create a flask app
\begin{lstlisting}[language=python, numbers=none]
app = Flask('ping')
\end{lstlisting}

and then add a decorator, which is a way to add some extra functionality to our functions. This extra functionality will allow us turn the function into a web service. We specify the address the 'ping' will live in, and the method to access this route. 
\begin{lstlisting}[language=python, numbers=none]
@app.route('/ping', methods=['GET'])
\end{lstlisting}

We run the app in debug mode and specify the host to run on.
\begin{lstlisting}[language=python, numbers=none]
if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=9696)
\end{lstlisting}

We get the following output when it runs:
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/ping.png}
	\caption{Ping result}
	\label{fig:9}
\end{figure}
\FloatBarrier
To test it, open your browser and search \smtt{localhost:9696/ping}, You'll see that the 'PONG' string is received. Congrats You've made a simple web server.

\subsection{Serving the Churn Model with Flask}
In serving the churn model with flask, we want the model to be available at \smtt{/predict} in the \texttt{churn service}. The \texttt{marketing service} will send the \texttt{churn service} with information about the customers, and then we reply them with the probability of churning. The \texttt{churn service} also sends a promotional email to the customer with 25\% discount. To make the web service predict the churn value for each customer, we need to first load the previous saved model and use a prediction function in a special route.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/deploy-flask.png}
	\caption{Ping result}
	\label{fig:10}
\end{figure}
\FloatBarrier
\begin{itemize}[noitemsep, topsep=0pt]
\item To load the previous saved model, we create a \texttt{predict.py} file that loads the model
\begin{lstlisting}[language=python, numbers=none]
# Load the model
import pickle
from flask import Flask
from flask import request
from flask import jsonify

model_file = 'model_C=1.0.bin'

# load the model
with open(model_file, 'rb') as f_in: # read binary file
    dv, model = pickle.load(f_in)    # load() loads the file 
\end{lstlisting}
\item A function to predict a value for a customer
\begin{lstlisting}[language=python, numbers=none]
def predict_single(customer, dv, model):
    X = dv.transform([customer])  # one-hot encoding to the data 
    y_pred = model.predict_proba(X)[:, 1]
    return y_pred[0]
\end{lstlisting}
\item Then the function used to create the web service that makes prediction and sends an email to the customer.
\begin{lstlisting}[language=python, numbers=none]
app = Flask('churn') # name of the app
@app.route('/predict', methods=['POST'])  # in order to send the customer information we need to post its data.
def predict():
    customer = request.get_json()  # web services work best with json frame
    prediction = predict_single(customer, dv, model)
    churn = prediction >= 0.5

    result = {
        'churn_probability': float(prediction),  # cast numpy float type to python native float type
        'churn': bool(churn),  # casting the value using bool method
    }
    return jsonify(result)  # send back the data in json format to the user
\end{lstlisting}
\end{itemize}

To deploy,  the customer information in the \texttt{deploy.py} file is sent as JSON. The predict script is turned into a web service and sends a response back to the \texttt{marketing service} with predictions as JSON and an email to the customer likely to churn.
\begin{lstlisting}[language=python, numbers=none]

customer_id = "asdx-123d"
customer_email = "asdx-123d@yahoo.com"
customer = {
    "gender": "female",
    "seniorcitizen": 0,
    "partner": "yes",
    "dependents": "no",
    "phoneservice": "no",
    "multiplelines": "no_phone_service",
    "internetservice": "dsl",
    "onlinesecurity": "no",
    "onlinebackup": "yes",
    "deviceprotection": "no",
    "techsupport": "no",
    "streamingtv": "no",
    "streamingmovies": "no",
    "contract": "two_year",
    "paperlessbilling": "yes",
    "paymentmethod": "electronic_check",
    "tenure": 10,
    "monthlycharges": 29.85,
    "totalcharges": (2 * 29.85)
}
# Making requests
import requests
url = "http://localhost:9696/predict"
response = requests.post(url, json=customer).json()
print(response)

if response["churn"]:
    print(f"Sending email to {customer_id} with email:", {customer_email})
else:
    print(f"Customer {customer_id} will not churn")
\end{lstlisting}

You can run the prediction app using:
\begin{lstlisting}[language=python, numbers=none]
python predict.py
\end{lstlisting}
When you run the app, you will see a warning that it is not a WGSI server and not suitable for production environments. To fix this issue and run this as a production server there are plenty of ways available. One way to create a WSGI server is to use gunicorn. To install it use the command 
\begin{lstlisting}[language=python, numbers=none]
pip install gunicorn
\end{lstlisting}
And to run the WGSI server you can simply run it with the command 
\begin{lstlisting}[language=python, numbers=none]
gunicorn --bind 0.0.0.0:9696 predict:app 
\end{lstlisting}

Note that in \texttt{predict:app} the name ``predict'' is the name we set for the file containing the code \texttt{app = Flask('churn')}(for example: predict.py), You may need to change it to whatever you named your Flask app file. So far, we have been able to make a production server that predict the churn value for new customers.

\subsection{Dependencies and Environment Management: Pipenv}
Every time we're running a file from a directory we're using the executive files from a global directory. For example when we install python on our machine the executable files that are able to run our codes will go to somewhere like \smtt{\textbf{/home/username/python/bin/}} for example the pip command may go to \smtt{\textbf{/home/username/python/bin/pip}}. Sometimes the versions of libraries conflict (the project may not run or get into massive errors). For example we have an old project that uses sklearn library with the version of 0.24.1 and now we want to run it using sklearn version 1.0.0. We may get into errors because of the version conflict.
To solve the conflict we can make virtual environments. Virtual environment is something that can seperate the libraries installed in our system and the libraries with specified version we want our project to run with. There are a lot of ways to create a virtual environments. 

One way we are going to use is using a library named \texttt{pipenv}.  ``pipenv'' is a library that can create a virutal environment. To install this library just use the classic method \smtt{pip install pipenv}. After installing \texttt{pipenv}, we must install the libraries we want for our project in the new virtual environment. It's really easy, Just use the command \texttt{pipenv} instead of \texttt{pip}. i.e., \smtt{\textbf{pipenv install numpy scikit-learn==0.24.2 flask gunicorn}}. With this command we installed the libraries we want for our project. Note that using the pipenv command we made two files named ``Pipfile'' and ``Pipfile.lock''. If we look at this files closely we can see that in Pipfile the libraries we installed are named. If we specified the library name, it's also specified in Pipfile.

The Pipfile looks like:
\begin{lstlisting}[language=python, numbers=none]
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
numpy = "*"
scikit-learn = "*"
flask = "*"
gunicorn = "*"

[dev-packages]

[requires]
python_version = "3.9"

\end{lstlisting}

In ``Pipfile.lock'' we can see that each library with it's installed version is named and a hash file is there to reproduce if we move the environment to another machine.
If we want to run the project in another machine, we can easily installed the libraries we want with the command \smtt{\textbf{pipenv install.}} This command will look into Pipfile and Pipfile.lock to install the libraries with specified version. After installing the required libraries we can run the project in the virtual environment with ``pipenv shell'' command. 
\begin{lstlisting}[language=python, numbers=none]
pipenv shell
\end{lstlisting}

This will go to the virtual environment's shell and then any command we execute will use the virtual environment's libraries. Installing and using the libraries such as gunicorn is the same as the last session.  To run the \smtt{predict} app, we can call gunicorn in the ``pipenv shell'' using:

\begin{lstlisting}[language=python, numbers=none]
gunicorn --bind 0.0.0.0:9696 predict:app
\end{lstlisting}

If you can't access the port, first kill any running process by checking the running processes with:
\begin{lstlisting}[language=python, numbers=none]
sudo lsof -t -i:9696
\end{lstlisting}

Then kill the port using:
\begin{lstlisting}[language=python, numbers=none]
kill <PID number>
\end{lstlisting}

After killing the port, you can try to run the ``predict'' app with gunicorn again. Until here we made a virtual environment for our libraries with a required specified version. To seperate this environment more, such as making gunicorn be able to run in windows machines we need another way. The other way is using \smtt{\textbf{Docker}}. Docker allows us to seperate everything more than before and make any project able to run on any machine that support Docker smoothly.

\subsection{Dependencies and Environment Management: Docker}
Let's say we have our host machine which is a laptop with Ubuntu and then on this laptop, we have different virtual environment. For instance, we may have a virtual environment for \texttt{churn service} and another one for \texttt{lead scoring service} with different python versions and dependencies. With Docker, we can get even more isolation. So instead of creating virtual environment for each services, we can put each service in a separate container and these services will not know anything about each other.

To perform more isolation .i.e,  separate our project file from our system machine, there is an option named Docker. With Docker you are able to pack all your project in a system that you want and run it in any system machine. For example if you want Ubuntu 20.4 you can have it in a mac or windows machine or other operating systems. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Images/docker-isolation.png}
	\caption{Docker Isolation}
	\label{fig:11}
\end{figure}
%\FloatBarrier

To get started with Docker for the churn prediction project you can follow the instructions below.

\begin{itemize}
\item On Ubuntu
\begin{lstlisting}[language=python, numbers=none]
sudo apt-get install docker.io
\end{lstlisting}

To run docker without sudo, follow this instruction.

\item On MacOS
	\begin{itemize}[noitemsep, topsep=0pt]
		\item Follow the steps in the \href{https://docs.docker.com/desktop/install/mac-install/}{Docker docs}.
	\end{itemize}
\end{itemize}

To run docker using the default command in python:
\begin{lstlisting}[language=python, numbers=none]
docker run -it --rm python:3.8.12-slim
\end{lstlisting}

We can change the entrypoint to be bash using:
\begin{lstlisting}[language=python, numbers=none]
docker run -it --rm --entrypoint=bash python:3.8.12-slim
\end{lstlisting}

Once our project was packed in a Docker container, we're able to run our project on any machine.  First we have to make a Docker image. In Docker image file, there are settings and dependencies we have in our project. To find Docker images that you need you can simply search the Docker website. 

We can create a Dockerfile where we define everything that we want to do in a docker image. Here is a Dockerfile (There should be no comments in Dockerfile, so remove the comments when you copy)

\begin{lstlisting}[language=python, numbers=none]
# First install the python 3.8, the slim version uses less space
FROM python:3.8.12-slim

# Install pipenv library in Docker 
RUN pip install pipenv

# create a directory in Docker named app and we're using it as work directory 
WORKDIR /app                                                                

# Copy the Pip files into our working derectory 
COPY ["Pipfile", "Pipfile.lock", "./"]

# install the pipenv dependencies for the project and deploy them.
RUN pipenv install --deploy --system

# Copy any python files and the model we had to the working directory of Docker 
COPY ["*.py", "model_C=1.0.bin", "./"]

# We need to expose the 9696 port because we're not able to communicate with Docker outside it
EXPOSE 9696

# If we run the Docker image, we want our churn app to be running
ENTRYPOINT ["gunicorn", "--bind", "0.0.0.0:9696", "predict:app"]
\end{lstlisting}

The flags \smtt{--deploy} and \smtt{--system} makes sure that we install the dependencies directly inside the Docker container without creating an additional virtual environment (which pipenv does by default).

If we don't put the last line ENTRYPOINT, we will be in a python shell. Note that for the entrypoint, we put our commands in double quotes.

After creating the Dockerfile, we need to build it:
\begin{lstlisting}[language=python, numbers=none]
docker build -t churn-prediction .
\end{lstlisting}

To run it, execute the command below:
\begin{lstlisting}[language=python, numbers=none]
docker run -it -p 9696:9696 churn-prediction:latest
\end{lstlisting}

Flag explanations:
\begin{itemize}[noitemsep, topsep=0pt]
\item \smtt{-t}: is used for specifying the tag name ``churn-prediction''.
\item \smtt{-it}: in order for Docker to allow us access to the terminal.
\item \smtt{--rm}: allows us to remove the image from the system after we're done.
\item \smtt{-p}: to map the 9696 port of the Docker to 9696 port of our machine. (first 9696 is the port number of our machine and the last one is Docker container port.)
\item \smtt{--entrypoint=bash}: After running Docker, we will now be able to communicate with the container using bash (as you would normally do with the Terminal). Default is python.
\end{itemize}

At last you've deployed your prediction app inside a Docker continer. 


\section{Online Deployment}
\subsection{Web services: Deploying models with Flask and Docker}
\subsubsection{Creating a Virtual Environment with Pipenv}
To deploy the model, we have to create a virtual environment using \texttt{Pipenv}. In your project folder, create a web-service folder.  This ``web-service'' will house the files required to deploy the application.  We should make sure that the version we used to create the pickle file is the exact same version we install in the virtual environment.
\begin{lstlisting}[language=python, numbers=none]
 pip freeze | grep scikit-learn
\end{lstlisting}

Using the same exact version of scikit-learn, we install that in the virtual environment with ``pipenv''. In the web-service folder, run the command to install the necessary packages using ``pipenv''.
\begin{lstlisting}[language=python, numbers=none]
 pipenv install scikit-learn==1.0.2 flask --python=3.9
\end{lstlisting}

This creates a \texttt{Pipfile} and \texttt{Pipfile.lock} containing the packages we just installed and their exact version. Once the virtual environment have been created, launch the subshell in virtual environment. To enter the virtual environment, run the command below:
\begin{lstlisting}[language=python, numbers=none]
pipenv shell
\end{lstlisting}
The prompt is usually long. We can reduce it using:
\begin{lstlisting}[language=python, numbers=none]
PS1="> "
\end{lstlisting}

\subsubsection{Creating a script for predicting}
To predict, we create a script for prediction called \texttt{predict.py}:

\begin{lstlisting}[language=python, numbers=none]
import pickle

with open('lin_reg.bin', 'rb') as f_in:
    (dv, model) = pickle.load(f_in)

def prepare_features(ride):
    features = {}
    features['PU_DO'] = '%s_%s' % (ride['PULocationID'], ride['DOLocationID'])
    features['trip_distance'] = ride['trip_distance']
    return features

def predict(features):
    X = dv.transform(features)
    preds = model.predict(X)
    return preds[0]
\end{lstlisting}

We can test our prediction with a \texttt{test.py} file
\begin{lstlisting}[language=python, numbers=none]
import predict

ride = {
    "PULocationID": 10,
    "DOLocationID": 50,
    "trip_distance": 40
}

features = predict.prepare_features(ride)
pred = predict.predict(ride)
print(pred)
\end{lstlisting}
To run prediction, we simply run \textbf{\smtt{python test.py}} in terminal.

\subsubsection{Putting the script into a Flask app}
To turn the scripts above to a flask application, we create a function that will be a wrapper around the \texttt{predict.py} and \texttt{test.py} file. This function will take web requests, HTTP request and the score it gets in the request and return the prediction. Let's call the function \texttt{predict\_endpoint()}. Update the \texttt{predict.py} file with the function \texttt{predict\_endpoint()}.

\begin{lstlisting}[language=python, numbers=none]
import pickle
from flask import Flask, request, jsonify

with open('lin_reg.bin', 'rb') as f_in:
    (dv, model) = pickle.load(f_in)

def prepare_features(ride):
    features = {}
    features['PU_DO'] = '%s_%s' % (ride['PULocationID'], ride['DOLocationID'])
    features['trip_distance'] = ride['trip_distance']
    return features

def predict(features):
    X = dv.transform(features)
    preds = model.predict(X)
    return float(preds[0])
  
app = Flask('duration-prediction')

@app.route('/predict', methods=['POST']) # turn predict function into an endpoint
def predict_endpoint():
    ride = request.get_json()

    features = prepare_features(ride)
    pred = predict(features)

    result = {
        'duration': pred
    }
    return jsonify(result)


if __name__ == "__main__":
    app.run(debug=True, host='0.0.0.0', port=9696)
\end{lstlisting}
In the above code, we added an import for Flask, requests and jsonify. Requests gives us a way get the data,  the result of our prediction will be a dictionary with duration and jsonify returns this as a \texttt{json} object. We also added a decorator which adds some extra functionality to the predict function by turning the function into HTTP endpoint and we can start sending requests to this endpoint and get responses.  This library has a method called ``POST'' that we use for sending post requests and here we need to specify the url end point and port to which we want to send the request.

We update the \texttt{test.py} file with the \textbf{\smtt{url}} variable and \textbf{\smtt{response}} variable.
\begin{lstlisting}[language=python, numbers=none]
import requests
import predict

ride = {
    "PULocationID": 10,
    "DOLocationID": 50,
    "trip_distance": 40
}

url = 'http://localhost:9696/predict'
response = requests.post(url, json=ride)
print(response.json())
\end{lstlisting}

The \texttt{requests} library is used for making HTTP requests in Python. The \textbf{\smtt{url}} variable contains the address of the local server endpoint \textbf{\smtt{/predict}} running on port 9696. The \textbf{\smtt{requests.post}} method is used to send a POST request to the \textbf{\smtt{url.}} The \textbf{\smtt{json}} parameter is used to send the \textbf{\smtt{ride}} dictionary as a JSON payload in the request body. The response from the server is stored in the \textbf{\smtt{response}} variable. The \textbf{\smtt{response.json()}} method is called to parse the JSON response from the server into a Python dictionary. The parsed response is then printed to the console.

The code in the \texttt{test.py} file sends a POST request to a local server to get a prediction based on the provided ride details. In the prompt in the virtual environment, we run \textbf{\texttt{python predict.py}} and open another terminal to run \textbf{\texttt{python test.py}}.  Now we have sucessfully put our model in a flask and deployed that model in a development environment. Alternatively, we can install gunicorn if we want to put this into production. The application we just ran is running in a development environment in Flask. To run it in a production environment, we use gunicorn.  We first install gunicorn

\begin{lstlisting}[language=python, numbers=none]
pipenv install gunicorn
\end{lstlisting}

Then start the service using:
\begin{lstlisting}[language=python, numbers=none]
gunicorn --bind 0.0.0.0:9696 predict:app 
\end{lstlisting}
Then test the the service and make prediction with \textbf{\texttt{python test.py}}.

\subsubsection{Packaging the app to Docker}
To package the app to \texttt{Docker}, first we need to create a ``Dockerfile''. In the Dockerfile, we add the following command:
\begin{lstlisting}[language=python, numbers=none]
FROM python:3.9.18-slim

RUN pip install -U pip
RUN pip install pipenv

WORKDIR /app  

COPY ["Pipfile", "Pipfile.lock", "./"]

RUN pipenv install --deploy --system

COPY ["predict.py", "lin_reg.bin", "./"]

EXPOSE 9696

ENTRYPOINT ["gunicorn", "--bind=0.0.0.0:9696", "predict:app"]
\end{lstlisting}

Next we build the docker image using the Dockerfile
\begin{lstlisting}[language=python, numbers=none]
docker build -t ride-duration-prediction-service:v1 .
\end{lstlisting}

And then we can test it in interactive mode (``--it'') and remove it after running (``-p'') using:
\begin{lstlisting}[language=python, numbers=none]
docker run -it --rm -p 9696:9696 ride-duration-prediction-service:v1 
\end{lstlisting}

In a separate shell, test the the service and make prediction with \textbf{\texttt{python test.py}}. With the docker container, we can serve our model and deploy the service anywhere docker is supported .e.g., using AWS Elastic Beanstalk or Kubernetes.

\subsection{Web services: Getting the models from the model registry (MLflow)}




%
\begin{itemize}
  \item Like this one,
    \begin{notebox}
      which is wrapped in gray. I use it for notes.\ldots
    \end{notebox}

  \item Or this one,
    \begin{funfact}
      which is wrapped in red. I use it for fun facts or other asides\ldots
    \end{funfact}

  \item Or this one,
    \begin{mathaside}
      which is wrapped in blue and used for mathy stuff.
    \end{mathaside}

  \item Or this last one,
    \begin{example}
      which is wrapped in green. With a title, it's used for enumerated examples
      (see \smtt{\textbackslash extitle} and \smtt{\textbackslash excounter}).
      Observe:
    \end{example}

    \begin{example}[frametitle=\extitle{Test}]
      This is an example. What's the answer to $2+2$?
      \answer{Obviously 4, lol.}
    \end{example}

    \begin{example}[frametitle=\extitle{Test Again}]
      This one will increment the counter automatically, resetting for each
      chapter.
    \end{example}


  \item For red and blue boxes, there are custom commands for titles, too:
    \begin{mathaside}[frametitle=\mathtitle{One Title}]
      Like this
    \end{mathaside}
    \begin{mathaside}[frametitle=\mathtitlep{Two Titles}{A Subtitle}]
      Or this
    \end{mathaside}
\end{itemize}

\hr{5in}

These styles also automatically apply to theorems and claims.

\begin{theorem}[Pythagorean Theorem]
  \label{thm:pyth}
  For any right triangle with legs $a,b$ and hypotenuse $c$:
  %
  \begin{equation}
    \label{eq:pyth}
    a^2+b^2=c^2
  \end{equation}
\end{theorem}
\begin{proof}
  This is left as an exercise to the reader.
\end{proof}

\begin{claim}
  This is the greatest note template in the world.
\end{claim}

\hr{5in}

There are different ways to quote things, too, depending on how you want to
emphasize:

\begin{quoting}
  This is a simple, indented quote with small letters and italics usually
  suitable for in-text quotations when you just want a block.
\end{quoting}

Alternatively, you can use the \smtt{\textbackslash inspiration} command from
the chapter heading, which leverages the \smtt{thickleftborder} frame
internally, but adds a little more padding and styling (there's also just
\smtt{leftborder} for a thinner variant):

\begin{thickleftborder}
  Hello there!
\end{thickleftborder}



\section{On Cross-Referencing}
\marginnote{\footnotesize\softtext This is the standard way to include margin
notes. There are also commands to link to source papers directly (see
\smtt{\textbackslash lesson}).} You can reference most things---see
\autoref{thm:pyth} or \eqref{eq:pyth} or the \nameref{ch:1} chapter---directly
and easily as long as you give them labels. These are ``built-ins.'' However,
you can also create a \term{custom term} that will be included in the index,
then include references to it that link back to the original definition. Try
clicking: \refterm{custom term}. Building the index is on you, though. You can
also reference by using a different term for the text: \reftermx{custom
term}{like this}. Sometimes it doesn't fit the \termx{grammar}{grammatical
structure} of the sentence so you can define the term one way and visualize it
another way (this creates a \aterm{grammar} entry in the index). There's also
\prop{math terms} and a way to reference them: \refprop{math terms} (clickable),
but they do \textbf{not} show up in the index.



\section{On Math}
Most of the math stuff is just macros for specific things like the convolution
operator, $\conv$, probabilities, $\cprob{A}{B=C}$, or big-$O$ notation,
$\bigO{n^2\log{n}}$ but there's also a convenient way to include explanations on
the side of an equation:
%
\begin{align*}
  1 + 1 &\overset{?}{=} 2    \sideblock{2in}{first we do this} \\
      2 &\overset{?}{=} 2    \sideblock{2in}{then we do this} \\
      2 &= 2 \qed
\end{align*}

These are all in the \smtt{CustomCommands.sty} file.



\end{document}

